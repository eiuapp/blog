<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2021-01-08 Fri 14:57 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ceph</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" title="Standard" href="/html/style/worg.css" type="text/css" />
<link rel="alternate stylesheet" title="Zenburn" href="/html/style/worg-zenburn.css" type="text/css" />
<link rel="alternate stylesheet" title="Classic" href="/html/style/worg-classic.css" type="text/css" />
<link rel="SHORTCUT ICON" href="/html/style/org-mode-unicorn.ico" type="image/x-icon" />
<link rel="icon" href="/html/style/org-mode-unicorn.ico" type="image/ico" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="preamble" class="status">
<script type="text/javascript">
  document.addEventListener('DOMContentLoaded',function() {
      document.getElementById("table-of-contents").onclick = function() {
          var elem = document.getElementById("text-table-of-contents");
          elem.style.display = elem.style.display == "block" ? "none" : "block";
      }
  });

  var url = document.location.href;
  var orgsource = url.substring(0, url.lastIndexOf("."))+".org.html";
  function show_org_source(){
      document.location.href = orgsource;
  }
</script>

<script>
(function(f, a, t, h, o, m){
	a[h]=a[h]||function(){
		(a[h].q=a[h].q||[]).push(arguments)
	};
	o=f.createElement('script'),
	m=f.getElementsByTagName('script')[0];
	o.async=1; o.src=t; o.id='fathom-script';
	m.parentNode.insertBefore(o,m)
})(document, window, '//stats.orgmode.org/tracker.js', 'fathom');
fathom('set', 'siteId', 'NWSQJ');
fathom('trackPageview');
</script>
</div>
<div id="content">
<h1 class="title">ceph</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org1b35dc4">Post</a>
<ul>
<li><a href="#org6dfd6ec">ceph install base ubuntu</a>
<ul>
<li><a href="#orga352b42">ceph 安装，基于ubuntu, intro</a></li>
<li><a href="#orge12cc6b">ceph-install-base-ubuntu READM</a></li>
<li><a href="#orgf476986">ceph-install-base-ubuntu quick-start-preflight</a></li>
<li><a href="#orga4caeb9">ceph-install-base-ubuntu Storage Cluster Quick Start</a></li>
<li><a href="#org0d3c14c">ceph-install-base-ubuntu Block Device Quick Start</a></li>
<li><a href="#org52e256d">ceph-install-base-ubuntu Filesystem Quick Start</a></li>
<li><a href="#orgd2e8af4">ceph-install-base-ubuntu Object Storage Quick Start</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org1b35dc4" class="outline-2">
<h2 id="org1b35dc4">Post</h2>
<div class="outline-text-2" id="text-org1b35dc4">
</div>
<div id="outline-container-org6dfd6ec" class="outline-3">
<h3 id="org6dfd6ec">ceph install base ubuntu&#xa0;&#xa0;&#xa0;<span class="tag"><span class="_ceph">@ceph</span>&#xa0;<span class="ceph">ceph</span>&#xa0;<span class="install">install</span>&#xa0;<span class="ubuntu">ubuntu</span></span></h3>
<div class="outline-text-3" id="text-org6dfd6ec">
</div>
<div id="outline-container-orga352b42" class="outline-4">
<h4 id="orga352b42">ceph 安装，基于ubuntu, intro&#xa0;&#xa0;&#xa0;<span class="tag"><span class="intro">intro</span></span></h4>
<div class="outline-text-4" id="text-orga352b42">
<p>
Contents:
</p>

<ul class="org-ul">
<li>README</li>
<li>Preflight</li>
<li>Storage Cluster Quick Start</li>
<li>Block Device Quick Start</li>
<li>Filesystem Quick Start</li>
<li>Object Storage Quick Start</li>
</ul>
</div>
</div>


<div id="outline-container-orge12cc6b" class="outline-4">
<h4 id="orge12cc6b">ceph-install-base-ubuntu READM&#xa0;&#xa0;&#xa0;<span class="tag"><span class="readme">readme</span></span></h4>
<div class="outline-text-4" id="text-orge12cc6b">
<p>
本次安装，完全按照 官方文档 <a href="http://docs.ceph.com/docs/master/start/">http://docs.ceph.com/docs/master/start/</a>
进行。
</p>
</div>

<ul class="org-ul">
<li><a id="env"></a>env<br />
<div class="outline-text-5" id="text-env">
<pre class="example">
192.168.31.115 cephadmin # 这个是admin节点
192.168.31.114 mon1 # 存储节点
192.168.31.113 cephfsn2 # 存储节点
192.168.31.173 cephfsn3  # 存储节点
192.168.31.172 ceph-client  # 客户端节点
</pre>

<p>
上面这几个，可以全部写进 /etc/hosts 中。且各节点的 hostname
必须对应相同（否则安装容易出错）。
</p>
</div>
</li>
</ul>
</div>
<div id="outline-container-orgf476986" class="outline-4">
<h4 id="orgf476986">ceph-install-base-ubuntu quick-start-preflight</h4>
<div class="outline-text-4" id="text-orgf476986">
<p>
这里的配置注意点如下：
</p>
</div>

<ul class="org-ul">
<li><a id="ceph-admin-节点"></a>ceph-admin 节点：<br />
<div class="outline-text-5" id="text-ceph-admin-节点">
<p>
安装时， with a stable Ceph release (e.g., luminous.) ，安装 luminous
或以上版本。
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ sudo cat /etc/apt/sources.list.d/ceph.list
deb https://download.ceph.com/debian-luminous/ xenial main
cephu@cephadmin:~/my-cluster$
</pre>

<p>
配置 ~/.ssh/config 方便后续安装
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ cat ~/.ssh/config
Host node0
    Hostname cephadmin
    User cephu
Host node1
    Hostname mon1
    User cephu
Host node2
    Hostname cephfsn2
    User cephu
Host node3
    Hostname cephfsn3
    User cephu
cephu@cephadmin:~/my-cluster$
</pre>

<p>
这里，我后来想了一下，应该把这个地方，修改成 全用 node0, node1, node2,
node3, 然后，各节点的　Hostname
也修改成这些，这样子才是真的方便安装。如下：
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ cat ~/.ssh/config
Host node0
    Hostname node0
    User cephu
Host node1
    Hostname node1
    User cephu
Host node2
    Hostname node2
    User cephu
Host node3
    Hostname node3
    User cephu
cephu@cephadmin:~/my-cluster$
</pre>
</div>
</li>

<li><a id="存储节点"></a>存储节点：<br />
<div class="outline-text-5" id="text-存储节点">
<pre class="example">
cephu@cephadmin:~/my-cluster$ cat /etc/hosts
127.0.0.1       localhost
127.0.1.1       ubuntu
#127.0.0.1      cephfs5

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

192.168.31.115 cephadmin
192.168.31.114 mon1
192.168.31.113 cephfsn2
192.168.31.173 cephfsn3
</pre>
</div>
</li>
</ul>
</div>

<div id="outline-container-orga4caeb9" class="outline-4">
<h4 id="orga4caeb9">ceph-install-base-ubuntu Storage Cluster Quick Start</h4>
<div class="outline-text-4" id="text-orga4caeb9">
<p>
2.1 Create the cluster.
</p>

<blockquote>
<p>
ceph-deploy new node1 不生效的，要写成 ceph-deploy new mon1
</p>
</blockquote>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy new mon1
</pre>

<p>
2.2 - 2.3
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ cat ~/my-cluster/ceph.conf
[global]
fsid = d0aa5af1-4f8e-4953-9448-7f1b2448b8a5
mon_initial_members = mon1
mon_host = 192.168.31.114
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx

public network = 192.168.31.1/24
ms bind ipv6 = true
cephu@cephadmin:~/my-cluster$
</pre>

<p>
2.4
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy install node1 node2 node3
</pre>

<p>
2.5
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy mon create-initial
</pre>

<p>
报错，然后，经过 农总在 <a href="https://my.oschina.net/u/2475751/blog/647777">https://my.oschina.net/u/2475751/blog/647777</a>
查到要运行下面
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ssh mon1 sudo ceph-create-keys --id mon1
</pre>

<p>
再来一次
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy mon create-initial
</pre>

<p>
3.1
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy --overwrite-conf admin node1 node2 node3
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephu/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.32): /usr/bin/ceph-deploy --overwrite-conf admin node1 node2 node3
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2c1fbe2440&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['node1', 'node2', 'node3']
[ceph_deploy.cli][INFO  ]  func                          : &lt;function admin at 0x7f2c20489b18&gt;
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node1
[node1][DEBUG ] connection detected need for sudo
[node1][DEBUG ] connected to host: node1
[node1][DEBUG ] detect platform information from remote host
[node1][DEBUG ] detect machine type
[node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node2
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node3
[node3][DEBUG ] connection detected need for sudo
[node3][DEBUG ] connected to host: node3
[node3][DEBUG ] detect platform information from remote host
[node3][DEBUG ] detect machine type
[node3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
cephu@cephadmin:~/my-cluster$
</pre>

<p>
3.2
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy mgr create node1
usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME]
                [--overwrite-conf] [--cluster NAME] [--ceph-conf CEPH_CONF]
                COMMAND ...
ceph-deploy: error: argument COMMAND: invalid choice: 'mgr' (choose from 'new', 'install', 'rgw', 'mon', 'mds', '                                                                                                                         gatherkeys', 'disk', 'osd', 'admin', 'repo', 'config', 'uninstall', 'purge', 'purgedata', 'calamari', 'forgetkeys                                                                                                                         ', 'pkg')
cephu@cephadmin:~/my-cluster$
</pre>

<p>
报错
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy -h
usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME]
                [--overwrite-conf] [--cluster NAME] [--ceph-conf CEPH_CONF]
                COMMAND ...

Easy Ceph deployment

-^-
/   \
|O o|  ceph-deploy v1.5.32
).-.(
'/|||\`
| '|` |
'|`

Full documentation can be found at: http://ceph.com/ceph-deploy/docs

optional arguments:
-h, --help            show this help message and exit
-v, --verbose         be more verbose
-q, --quiet           be less verbose
--version             the current installed version of ceph-deploy
--username USERNAME   the username to connect to the remote host
--overwrite-conf      overwrite an existing conf file on remote host (if
                        present)
--cluster NAME        name of the cluster
--ceph-conf CEPH_CONF
                        use (or reuse) a given ceph.conf file

commands:
COMMAND               description
new                 Start deploying a new cluster, and write a
                        CLUSTER.conf and keyring for it.
install             Install Ceph packages on remote hosts.
rgw                 Ceph RGW daemon management
mon                 Ceph MON Daemon management
mds                 Ceph MDS daemon management
gatherkeys          Gather authentication keys for provisioning new nodes.
disk                Manage disks on a remote host.
osd                 Prepare a data disk on remote host.
admin               Push configuration and client.admin key to a remote
                        host.
repo                Repo definition management
config              Copy ceph.conf to/from remote host(s)
uninstall           Remove Ceph packages from remote hosts.
purge               Remove Ceph packages from remote hosts and purge all
                        data.
purgedata           Purge (delete, destroy, discard, shred) any Ceph data
                        from /var/lib/ceph
calamari            Install and configure Calamari nodes. Assumes that a
                        repository with Calamari packages is already
                        configured. Refer to the docs for examples
                        (http://ceph.com/ceph-deploy/docs/conf.html)
forgetkeys          Remove authentication keys from the local directory.
pkg                 Manage packages on remote hosts.
cephu@cephadmin:~/my-cluster$
</pre>

<p>
我去，真的没有 mgr 命令。
</p>

<p>
版本升级吧
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ pip install ceph-deploy
Collecting ceph-deploy
Downloading ceph-deploy-1.5.39.tar.gz (114kB)
100% |████████████████████████████████| 122kB 292kB/s
Collecting setuptools (from ceph-deploy)
Downloading setuptools-36.6.0-py2.py3-none-any.whl (481kB)
100% |████████████████████████████████| 481kB 968kB/s
Building wheels for collected packages: ceph-deploy
Running setup.py bdist_wheel for ceph-deploy ... done
Stored in directory: /home/cephu/.cache/pip/wheels/5e/4a/c5/5759b04fedf1eaa17d4453b562ab28a2142dbf93ced0c37e5d
Successfully built ceph-deploy
Installing collected packages: setuptools, ceph-deploy
Successfully installed ceph-deploy-1.5.32 setuptools-20.7.0
You are using pip version 8.1.1, however version 9.0.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
cephu@cephadmin:~/my-cluster$ ceph-deploy --version
1.5.39
cephu@cephadmin:~/my-cluster$ ceph-deploy -h
</pre>

<p>
这下有了。重新来一下
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy mgr create node1
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephu/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mgr create node1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  mgr                           : [('node1', 'node1')]
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa24a321ab8&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : &lt;function mgr at 0x7fa24a993578&gt;
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.mgr][DEBUG ] Deploying mgr, cluster ceph hosts node1:node1
[ceph_deploy][ERROR ] RuntimeError: bootstrap-mgr keyring not found; run 'gatherkeys'
</pre>

<p>
哇，又来错误。
</p>

<p>
提示了 run 'gatherkeys' 。
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy gatherkeys mon1
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephu/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy gatherkeys mon1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbfda5f8a70&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  mon                           : ['mon1']
[ceph_deploy.cli][INFO  ]  func                          : &lt;function gatherkeys at 0x7fbfda8570c8&gt;
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpZ8THzr
[mon1][DEBUG ] connection detected need for sudo
[mon1][DEBUG ] connected to host: mon1
[mon1][DEBUG ] detect platform information from remote host
[mon1][DEBUG ] detect machine type
[mon1][DEBUG ] get remote short hostname
[mon1][DEBUG ] fetch remote file
[mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
[ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpZ8THzr
cephu@cephadmin:~/my-cluster$
</pre>

<p>
再走一个
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy mgr create node1
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephu/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mgr create node1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  mgr                           : [('node1', 'node1')]
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5d02a1bab8&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : &lt;function mgr at 0x7f5d0308d578&gt;
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.mgr][DEBUG ] Deploying mgr, cluster ceph hosts node1:node1
[node1][DEBUG ] connection detected need for sudo
[node1][DEBUG ] connected to host: node1
[node1][DEBUG ] detect platform information from remote host
[node1][DEBUG ] detect machine type
[ceph_deploy.mgr][INFO  ] Distro info: Ubuntu 16.04 xenial
[ceph_deploy.mgr][DEBUG ] remote host will use systemd
[ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to node1
[node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.mgr][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 MGRs
</pre>

<p>
提示了 use &#x2013;overwrite-conf
</p>

<p>
再来
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy --overwrite-conf mgr create mon1
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephu/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf mgr create mon1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  mgr                           : [('mon1', 'mon1')]
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe415166ab8&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : &lt;function mgr at 0x7fe4157d8578&gt;
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.mgr][DEBUG ] Deploying mgr, cluster ceph hosts mon1:mon1
[mon1][DEBUG ] connection detected need for sudo
[mon1][DEBUG ] connected to host: mon1
[mon1][DEBUG ] detect platform information from remote host
[mon1][DEBUG ] detect machine type
[ceph_deploy.mgr][INFO  ] Distro info: Ubuntu 16.04 xenial
[ceph_deploy.mgr][DEBUG ] remote host will use systemd
[ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to mon1
[mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[mon1][WARNIN] mgr keyring does not exist yet, creating one
[mon1][DEBUG ] create a keyring file
[mon1][DEBUG ] create path if it doesn't exist
[mon1][INFO  ] Running command: sudo ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.mon1 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-mon1/keyring
[mon1][INFO  ] Running command: sudo systemctl enable ceph-mgr@mon1
[mon1][WARNIN] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@mon1.service to /lib/systemd/system/ceph-mgr@.service.
[mon1][INFO  ] Running command: sudo systemctl start ceph-mgr@mon1
[mon1][INFO  ] Running command: sudo systemctl enable ceph.target
cephu@cephadmin:~/my-cluster$
</pre>

<p>
终于成功了。
</p>

<p>
3.3
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy --overwrite-conf osd create node1:sdb node2:sdb node3:sdc
...
[node3][DEBUG ] Warning: The kernel is still using the old partition table.
[node3][DEBUG ] The new table will be used at the next reboot or after you
[node3][DEBUG ] run partprobe(8) or kpartx(8)
[node3][DEBUG ] The operation has completed successfully.
[node3][WARNIN] update_partition: Calling partprobe on prepared device /dev/sdc
[node3][WARNIN] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[node3][WARNIN] command: Running command: /usr/bin/flock -s /dev/sdc /sbin/partprobe /dev/sdc
[node3][WARNIN] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[node3][WARNIN] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match sdc1
[node3][INFO  ] Running command: sudo systemctl enable ceph.target
[node3][INFO  ] checking OSD status...
[node3][DEBUG ] find the location of an executable
[node3][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[node3][WARNIN] there is 1 OSD down
[node3][WARNIN] there is 1 OSD out
[ceph_deploy.osd][DEBUG ] Host node3 is now ready for osd use.
cephu@cephadmin:~/my-cluster$
</pre>

<p>
成功
</p>

<p>
3.4
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ssh node1 sudo ceph health
HEALTH_OK
cephu@cephadmin:~/my-cluster$ ssh node2 sudo ceph health
HEALTH_OK
cephu@cephadmin:~/my-cluster$ ssh node3 sudo ceph health
HEALTH_OK
cephu@cephadmin:~/my-cluster$ ssh node3 sudo ceph -s
cluster:
id:     d0aa5af1-4f8e-4953-9448-7f1b2448b8a5
health: HEALTH_OK

services:
mon: 1 daemons, quorum mon1
mgr: mon1(active)
osd: 3 osds: 3 up, 3 in

data:
pools:   0 pools, 0 pgs
objects: 0 objects, 0 bytes
usage:   3164 MB used, 80500 MB / 83664 MB avail
pgs:

cephu@cephadmin:~/my-cluster$
</pre>

<p>
到这里，我们应该是完成了基础的搭建
</p>
</div>

<ul class="org-ul">
<li><a id="附加项expanding-your-cluster"></a>附加项：expanding your cluster<br />
<div class="outline-text-5" id="text-附加项expanding-your-cluster">
</div>

<ul class="org-ul">
<li><a id="add-a-metadata-server"></a>ADD A METADATA SERVER<br />
<div class="outline-text-6" id="text-add-a-metadata-server">
<p>
ceph-deploy mds create node1
</p>
</div>
</li>

<li><a id="adding-monitors"></a>ADDING MONITORS<br />
<div class="outline-text-6" id="text-adding-monitors">
<p>
ceph-deploy mon add node2 node3 会报错
</p>

<p>
换成：
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy mon add cephfsn2
cephu@cephadmin:~/my-cluster$ ceph-deploy mon add cephfsn3
</pre>

<p>
验证
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ssh cephfsn2 ceph quorum_status --format json-pretty
2017-10-24 10:40:24.959942 7f261141a700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory
2017-10-24 10:40:24.959973 7f261141a700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication
2017-10-24 10:40:24.959975 7f261141a700  0 librados: client.admin initialization error (2) No such file or directory
[errno 2] error connecting to the cluster
cephu@cephadmin:~/my-cluster$ ssh cephfsn2 sudo ceph quorum_status --format json-pretty

{
"election_epoch": 14,
"quorum": [
        0,
        1,
        2
],
"quorum_names": [
        "cephfsn2",
        "mon1",
        "cephfsn3"
],
"quorum_leader_name": "cephfsn2",
"monmap": {
        "epoch": 3,
        "fsid": "d0aa5af1-4f8e-4953-9448-7f1b2448b8a5",
        "modified": "2017-10-24 10:32:19.273831",
        "created": "2017-10-23 15:22:32.766470",
        "features": {
        "persistent": [
                "kraken",
                "luminous"
        ],
        "optional": []
        },
        "mons": [
        {
                "rank": 0,
                "name": "cephfsn2",
                "addr": "192.168.31.113:6789/0",
                "public_addr": "192.168.31.113:6789/0"
        },
        {
                "rank": 1,
                "name": "mon1",
                "addr": "192.168.31.114:6789/0",
                "public_addr": "192.168.31.114:6789/0"
        },
        {
                "rank": 2,
                "name": "cephfsn3",
                "addr": "192.168.31.173:6789/0",
                "public_addr": "192.168.31.173:6789/0"
        }
        ]
}
}
cephu@cephadmin:~/my-cluster$
</pre>
</div>
</li>

<li><a id="adding-managers"></a>ADDING MANAGERS<br />
<div class="outline-text-6" id="text-adding-managers">
<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy mgr create node2 node3
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephu/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mgr create node2 node3
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  mgr                           : [('node2', 'node2'), ('node3', 'node3')]
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb9043a4ab8&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : &lt;function mgr at 0x7fb904a16578&gt;
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.mgr][DEBUG ] Deploying mgr, cluster ceph hosts node2:node2 node3:node3
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.mgr][INFO  ] Distro info: Ubuntu 16.04 xenial
[ceph_deploy.mgr][DEBUG ] remote host will use systemd
[ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node2][WARNIN] mgr keyring does not exist yet, creating one
[node2][DEBUG ] create a keyring file
[node2][DEBUG ] create path if it doesn't exist
[node2][INFO  ] Running command: sudo ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.node2 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-node2/keyring
[node2][INFO  ] Running command: sudo systemctl enable ceph-mgr@node2
[node2][WARNIN] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@node2.service to /lib/systemd/system/ceph-mgr@.service.
[node2][INFO  ] Running command: sudo systemctl start ceph-mgr@node2
[node2][INFO  ] Running command: sudo systemctl enable ceph.target
[node3][DEBUG ] connection detected need for sudo
[node3][DEBUG ] connected to host: node3
[node3][DEBUG ] detect platform information from remote host
[node3][DEBUG ] detect machine type
[ceph_deploy.mgr][INFO  ] Distro info: Ubuntu 16.04 xenial
[ceph_deploy.mgr][DEBUG ] remote host will use systemd
[ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to node3
[node3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node3][WARNIN] mgr keyring does not exist yet, creating one
[node3][DEBUG ] create a keyring file
[node3][DEBUG ] create path if it doesn't exist
[node3][INFO  ] Running command: sudo ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.node3 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-node3/keyring
[node3][INFO  ] Running command: sudo systemctl enable ceph-mgr@node3
[node3][WARNIN] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@node3.service to /lib/systemd/system/ceph-mgr@.service.
[node3][INFO  ] Running command: sudo systemctl start ceph-mgr@node3
[node3][INFO  ] Running command: sudo systemctl enable ceph.target
cephu@cephadmin:~/my-cluster$
</pre>

<p>
验证
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ssh node1 sudo ceph -s
cluster:
id:     d0aa5af1-4f8e-4953-9448-7f1b2448b8a5
health: HEALTH_WARN
        clock skew detected on mon.cephfsn3

services:
mon: 3 daemons, quorum cephfsn2,mon1,cephfsn3
mgr: mon1(active), standbys: node2, node3
osd: 3 osds: 3 up, 3 in

data:
pools:   0 pools, 0 pgs
objects: 0 objects, 0 bytes
usage:   3164 MB used, 80500 MB / 83664 MB avail
pgs:

cephu@cephadmin:~/my-cluster$ ssh node3 sudo ceph -s
cluster:
id:     d0aa5af1-4f8e-4953-9448-7f1b2448b8a5
health: HEALTH_WARN
        clock skew detected on mon.cephfsn3

services:
mon: 3 daemons, quorum cephfsn2,mon1,cephfsn3
mgr: mon1(active), standbys: node2, node3
osd: 3 osds: 3 up, 3 in

data:
pools:   0 pools, 0 pgs
objects: 0 objects, 0 bytes
usage:   3164 MB used, 80500 MB / 83664 MB avail
pgs:

cephu@cephadmin:~/my-cluster$
</pre>
</div>
</li>

<li><a id="add-an-rgw-instance"></a>ADD AN RGW INSTANCE<br />
<div class="outline-text-6" id="text-add-an-rgw-instance">
<pre class="example">
ceph-deploy rgw create node1
</pre>

<p>
好了。
</p>
</div>
</li>
</ul>
</li>

<li><a id="storingretrieving-object-data"></a>STORING/RETRIEVING OBJECT DATA<br />
<div class="outline-text-5" id="text-storingretrieving-object-data">
<p>
这一个小节，现在先不动。
</p>

<p>
算了，还是动吧。
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ sudo apt install ceph-common -y
cephu@cephadmin:~/my-cluster$ ls /etc/ceph/*
cephu@cephadmin:~/my-cluster$ ceph-deploy admin cephadmin
cephu@cephadmin:~/my-cluster$ ls /etc/ceph/*  ## 这里可以看到新增3个文件
cephu@cephadmin:~/my-cluster$ ceph-deploy gatherkeys cephadmin
</pre>

<p>
在 admin 与 ceph-client 节点，都运行一下。
</p>

<pre class="example">
jlch@k-m:/etc/apt/sources.list.d$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring
</pre>

<p>
现在可以开始执行命令了
</p>

<pre class="example">
jlch@k-m:/etc/apt/sources.list.d$ echo {Test-data} &gt; testfile.txt
jlch@k-m:/etc/apt/sources.list.d$ ceph osd pool create mytest 8
jlch@k-m:/etc/apt/sources.list.d$ rados put test-object-1 testfile.txt --pool=mytest
jlch@k-m:/etc/apt/sources.list.d$ rados -p mytest ls
jlch@k-m:/etc/apt/sources.list.d$ ceph osd map mytest test-object-1
</pre>
</div>
</li>
</ul>
</div>
<div id="outline-container-org0d3c14c" class="outline-4">
<h4 id="org0d3c14c">ceph-install-base-ubuntu Block Device Quick Start</h4>
<div class="outline-text-4" id="text-org0d3c14c">
</div>

<ul class="org-ul">
<li><a id="env"></a>env<br />
<div class="outline-text-5" id="text-env">
<p>
192.168.31.172 ceph-client
</p>

<p>
修改 hostname 为 ceph-client
</p>
</div>
</li>

<li><a id="install-ceph"></a>INSTALL CEPH<br />
<div class="outline-text-5" id="text-install-ceph">
<p>
admin 节点：
</p>

<pre class="example">
ceph-deploy install ceph-client
ceph-deploy admin ceph-client
</pre>

<p>
ceph-client 节点：
</p>

<pre class="example">
sudo chmod +r /etc/ceph/ceph.client.admin.keyring
</pre>
</div>
</li>

<li><a id="create-a-block-device-pool"></a>CREATE A BLOCK DEVICE POOL<br />
<div class="outline-text-5" id="text-create-a-block-device-pool">
<p>
admin 节点：
</p>

<p>
原来在 admin 节点，是没有这个 rbd 命令的。
</p>

<p>
要通过 sudo apt install ceph-common 之后，才会有。所以先运行一下。
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ sudo apt install ceph-common -y
cephu@cephadmin:~/my-cluster$ rbd pool init jlch
2017-10-24 15:57:45.951917 7fde4fa6e0c0 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory
2017-10-24 15:57:45.951937 7fde4fa6e0c0 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication
2017-10-24 15:57:45.951942 7fde4fa6e0c0  0 librados: client.admin initialization error (2) No such file or directory
rbd: couldn't connect to the cluster!
cephu@cephadmin:~/my-cluster$
</pre>

<p>
ceph-client 节点：
</p>

<pre class="example">
jlch@k-m:/etc/apt/sources.list.d$ sudo ls /etc/ceph/* -l
[sudo] password for jlch:
-rw-r--r-- 1 root root  63 Oct 24 15:32 /etc/ceph/ceph.client.admin.keyring
-rw-r--r-- 1 root root 249 Oct 24 15:32 /etc/ceph/ceph.conf
-rw-r--r-- 1 root root  92 Apr 21  2017 /etc/ceph/rbdmap
-rw------- 1 root root   0 Oct 24 11:20 /etc/ceph/tmp2IJh4C
jlch@k-m:/etc/apt/sources.list.d$
</pre>

<p>
我去，明明有的，为什么说 unable to find a keyring on
*******，哪里出的问题?
</p>

<p>
哈哈，这个问题的原因找到了。需要把 上一小节的 最后一个部分，执行一下。
</p>

<p>
而且，要按上一小节的方式，osd create 一个 pool 出来，才能在这里 pool
init .
</p>

<p>
比如之前就是创建了 pool mytest. 这里就是
</p>

<pre class="example">
rbd pool init mytest
</pre>
</div>
</li>

<li><a id="configure-a-block-device"></a>CONFIGURE A BLOCK DEVICE<br />
<div class="outline-text-5" id="text-configure-a-block-device">
<p>
On the ceph-client node
</p>

<pre class="example">
cephu@ceph-client:~$ rbd create mytest --size 4096 -m mon1 -k /etc/ceph/ceph.client.admin.keyring
rbd: error opening default pool 'rbd'
Ensure that the default pool has been created or specify an alternate pool name.
cephu@ceph-client:~$
</pre>

<p>
报错，因为，这里说了，opening default pool 'rbd'
出错，为啥？因为之前没有建立过 pool rbd 呀，之前，只有 pool mytest 呀。
怎么办？2个方法。1）重新新建立一个 pool rbd, 2）指向到已建立的pool, 如
pool mytest 。
</p>

<p>
我们走 方法2.
</p>

<p>
方法1
</p>

<p>
回 ceph-admin 节点：
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph osd pool create rbd 8
pool 'rbd' created
cephu@cephadmin:~/my-cluster$ rbd pool init rbd
cephu@cephadmin:~/my-cluster$
</pre>

<p>
回 ceph-client 节点：
</p>

<pre class="example">
cephu@ceph-client:~$ rbd create foo --size 4096 -m mon1 -k /etc/ceph/ceph.client.admin.keyring
</pre>

<p>
方法2
</p>

<pre class="example">
cephu@ceph-client:~$ rbd help create # 这里会发现是加 -p 参数来指定 pool
cephu@ceph-client:~$ rbd create foo --size 4096 -m mon1 -k /etc/ceph/ceph.client.admin.keyring -p mytest
cephu@ceph-client:~$
</pre>

<p>
好了，可以走下一步了。 我们继续。
</p>

<p>
On the ceph-client node, map the image to a block device.
</p>

<pre class="example">
cephu@ceph-client:~$ sudo rbd map foo --image client.admin -m mon1  -p mytest
rbd: sysfs write failed
rbd: error opening image client.admin: (2) No such file or directory
In some cases useful info is found in syslog - try "dmesg | tail".
rbd: map failed: (110) Connection timed out
cephu@ceph-client:~$
</pre>

<p>
报错了。
</p>

<pre class="example">
cephu@ceph-client:~$ dmesg | tail -n 100
...
[692522.117250] libceph: mon0 192.168.31.114:6789 missing required protocol features
[692532.096436] libceph: mon0 192.168.31.114:6789 feature set mismatch, my 106b84a842a42 &lt; server's 40106b84a842a42, missing 400000000000000
[692532.099897] libceph: mon0 192.168.31.114:6789 missing required protocol features
[692542.111938] libceph: mon0 192.168.31.114:6789 feature set mismatch, my 106b84a842a42 &lt; server's 40106b84a842a42, missing 400000000000000
[692542.115603] libceph: mon0 192.168.31.114:6789 missing required protocol features
</pre>

<p>
<a href="http://www.hl10502.com/2017/08/01/ceph-rbdmap-error-1/">http://www.hl10502.com/2017/08/01/ceph-rbdmap-error-1/</a>
</p>

<p>
上面有一些详细的原理
</p>

<p>
又要农总出马了。
</p>

<p>
<a href="http://blog.csdn.net/lk142500/article/details/78275910">http://blog.csdn.net/lk142500/article/details/78275910</a>
</p>

<p>
这里指出了解决方法。来吧。
</p>

<p>
admin 节点
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph -v
ceph version 12.2.1 (3e7492b9ada8bdc9a5cd0feafd42fbca27f9c38e) luminous (stable)
cephu@cephadmin:~/my-cluster$ ceph osd crush tunables optimal
adjusted tunables profile to optimal
cephu@cephadmin:~/my-cluster$ ceph osd crush rule ls
replicated_rule
cephu@cephadmin:~/my-cluster$ ceph osd crush rule dump
[
    {
        "rule_id": 0,
        "rule_name": "replicated_rule",
        "ruleset": 0,
        "type": 1,
        "min_size": 1,
        "max_size": 10,
        "steps": [
            {
                "op": "take",
                "item": -1,
                "item_name": "default"
            },
            {
                "op": "chooseleaf_firstn",
                "num": 0,
                "type": "host"
            },
            {
                "op": "emit"
            }
        ]
    }
]

cephu@cephadmin:~/my-cluster$ ceph osd crush show-tunables
{
    "choose_local_tries": 0,
    "choose_local_fallback_tries": 0,
    "choose_total_tries": 50,
    "chooseleaf_descend_once": 1,
    "chooseleaf_vary_r": 1,
    "chooseleaf_stable": 1,
    "straw_calc_version": 1,
    "allowed_bucket_algs": 54,
    "profile": "jewel",
    "optimal_tunables": 1,
    "legacy_tunables": 0,
    "minimum_required_version": "jewel",
    "require_feature_tunables": 1,
    "require_feature_tunables2": 1,
    "has_v2_rules": 0,
    "require_feature_tunables3": 1,
    "has_v3_rules": 0,
    "has_v4_buckets": 1,
    "require_feature_tunables5": 1,
    "has_v5_rules": 0
}

cephu@cephadmin:~/my-cluster$ ceph osd crush -h

General usage:
==============
usage: ceph [-h] [-c CEPHCONF] [-i INPUT_FILE] [-o OUTPUT_FILE]
            [--id CLIENT_ID] [--name CLIENT_NAME] [--cluster CLUSTER]
            [--admin-daemon ADMIN_SOCKET] [-s] [-w] [--watch-debug]
            [--watch-info] [--watch-sec] [--watch-warn] [--watch-error]
            [--watch-channel WATCH_CHANNEL] [--version] [--verbose]
            [--concise] [-f {json,json-pretty,xml,xml-pretty,plain}]
            [--connect-timeout CLUSTER_TIMEOUT]

Ceph administration tool

optional arguments:
-h, --help            request mon help
-c CEPHCONF, --conf CEPHCONF
                        ceph configuration file
-i INPUT_FILE, --in-file INPUT_FILE
                        input file, or "-" for stdin
-o OUTPUT_FILE, --out-file OUTPUT_FILE
                        output file, or "-" for stdout
--id CLIENT_ID, --user CLIENT_ID
                        client id for authentication
--name CLIENT_NAME, -n CLIENT_NAME
                        client name for authentication
--cluster CLUSTER     cluster name
--admin-daemon ADMIN_SOCKET
                        submit admin-socket commands ("help" for help
-s, --status          show cluster status
-w, --watch           watch live cluster changes
--watch-debug         watch debug events
--watch-info          watch info events
--watch-sec           watch security events
--watch-warn          watch warn events
--watch-error         watch error events
--watch-channel WATCH_CHANNEL
                        which log channel to follow when using -w/--watch. One
                        of ['cluster', 'audit', '*'
--version, -v         display version
--verbose             make verbose
--concise             make less verbose
-f {json,json-pretty,xml,xml-pretty,plain}, --format {json,json-pretty,xml,xml-pretty,plain}
--connect-timeout CLUSTER_TIMEOUT
                        set a timeout for connecting to the cluster

Local commands:
===============

ping &lt;mon.id&gt;           Send simple presence/life test to a mon
                        &lt;mon.id&gt; may be 'mon.*' for all mons
daemon {type.id|path} &lt;cmd&gt;
                        Same as --admin-daemon, but auto-find admin socket
daemonperf {type.id | path} [stat-pats] [priority] [&lt;interval&gt;] [&lt;count&gt;]
daemonperf {type.id | path} list|ls [stat-pats] [priority]
                        Get selected perf stats from daemon/admin socket
                        Optional shell-glob comma-delim match string stat-pats
                        Optional selection priority (can abbreviate name):
                        critical, interesting, useful, noninteresting, debug
                        List shows a table of all available stats
                        Run &lt;count&gt; times (default forever),
                        once per &lt;interval&gt; seconds (default 1)


Monitor commands:
=================
osd crush add &lt;osdname (id|osd.id)&gt; &lt;float[0.0-]&gt; &lt;args&gt; [&lt;args&gt;...]                                                 add or update crushmap position and weight for &lt;name&gt; with &lt;weight&gt; and location &lt;args&gt;
osd crush add-bucket &lt;name&gt; &lt;type&gt;                                                                                   add no-parent (probably root) crush bucket &lt;name&gt; of type &lt;type&gt;
osd crush class ls                                                                                                   list all crush device classes
osd crush class ls-osd &lt;class&gt;                                                                                       list all osds belonging to the specific &lt;class&gt;
osd crush class rename &lt;srcname&gt; &lt;dstname&gt;                                                                           rename crush device class &lt;srcname&gt; to &lt;dstname&gt;
osd crush create-or-move &lt;osdname (id|osd.id)&gt; &lt;float[0.0-]&gt; &lt;args&gt; [&lt;args&gt;...]                                      create entry or move existing entry for &lt;name&gt; &lt;weight&gt; at/to location &lt;args&gt;
osd crush dump                                                                                                       dump crush map
osd crush get-tunable straw_calc_version                                                                             get crush tunable &lt;tunable&gt;
osd crush link &lt;name&gt; &lt;args&gt; [&lt;args&gt;...]                                                                             link existing entry for &lt;name&gt; under location &lt;args&gt;
osd crush ls &lt;node&gt;                                                                                                  list items beneath a node in the CRUSH tree
osd crush move &lt;name&gt; &lt;args&gt; [&lt;args&gt;...]                                                                             move existing entry for &lt;name&gt; to location &lt;args&gt;
osd crush remove &lt;name&gt; {&lt;ancestor&gt;}                                                                                 remove &lt;name&gt; from crush map (everywhere, or just at &lt;ancestor&gt;)
osd crush rename-bucket &lt;srcname&gt; &lt;dstname&gt;                                                                          rename bucket &lt;srcname&gt; to &lt;dstname&gt;
osd crush reweight &lt;name&gt; &lt;float[0.0-]&gt;                                                                              change &lt;name&gt;'s weight to &lt;weight&gt; in crush map
osd crush reweight-all                                                                                               recalculate the weights for the tree to ensure they sum correctly
osd crush reweight-subtree &lt;name&gt; &lt;float[0.0-]&gt;                                                                      change all leaf items beneath &lt;name&gt; to &lt;weight&gt; in crush map
osd crush rm &lt;name&gt; {&lt;ancestor&gt;}                                                                                     remove &lt;name&gt; from crush map (everywhere, or just at &lt;ancestor&gt;)
osd crush rm-device-class &lt;ids&gt; [&lt;ids&gt;...]                                                                           remove class of the osd(s) &lt;id&gt; [&lt;id&gt;...],or use &lt;all|any|*&gt; to remove all.
osd crush rule create-erasure &lt;name&gt; {&lt;profile&gt;}                                                                     create crush rule &lt;name&gt; for erasure coded pool created with &lt;profile&gt; (default default)
osd crush rule create-replicated &lt;name&gt; &lt;root&gt; &lt;type&gt; {&lt;class&gt;}                                                      create crush rule &lt;name&gt; for replicated pool to start from &lt;root&gt;, replicate across buckets of type &lt;type&gt;, using a
                                                                                                                    choose mode of &lt;firstn|indep&gt; (default firstn; indep best for erasure pools)
osd crush rule create-simple &lt;name&gt; &lt;root&gt; &lt;type&gt; {firstn|indep}                                                     create crush rule &lt;name&gt; to start from &lt;root&gt;, replicate across buckets of type &lt;type&gt;, using a choose mode of
                                                                                                                    &lt;firstn|indep&gt; (default firstn; indep best for erasure pools)
osd crush rule dump {&lt;name&gt;}                                                                                         dump crush rule &lt;name&gt; (default all)
osd crush rule ls                                                                                                    list crush rules
osd crush rule ls-by-class &lt;class&gt;                                                                                   list all crush rules that reference the same &lt;class&gt;
osd crush rule rename &lt;srcname&gt; &lt;dstname&gt;                                                                            rename crush rule &lt;srcname&gt; to &lt;dstname&gt;
osd crush rule rm &lt;name&gt;                                                                                             remove crush rule &lt;name&gt;
osd crush set &lt;osdname (id|osd.id)&gt; &lt;float[0.0-]&gt; &lt;args&gt; [&lt;args&gt;...]                                                 update crushmap position and weight for &lt;name&gt; to &lt;weight&gt; with location &lt;args&gt;
osd crush set {&lt;int&gt;}                                                                                                set crush map from input file
osd crush set-device-class &lt;class&gt; &lt;ids&gt; [&lt;ids&gt;...]                                                                  set the &lt;class&gt; of the osd(s) &lt;id&gt; [&lt;id&gt;...],or use &lt;all|any|*&gt; to set all.
osd crush set-tunable straw_calc_version &lt;int&gt;                                                                       set crush tunable &lt;tunable&gt; to &lt;value&gt;
osd crush show-tunables                                                                                              show current crush tunables
osd crush swap-bucket &lt;source&gt; &lt;dest&gt; {--yes-i-really-mean-it}                                                       swap existing bucket contents from (orphan) bucket &lt;source&gt; and &lt;target&gt;
osd crush tree {--show-shadow}                                                                                       dump crush buckets and items in a tree view
osd crush tunables legacy|argonaut|bobtail|firefly|hammer|jewel|optimal|default                                      set crush tunables values to &lt;profile&gt;
osd crush unlink &lt;name&gt; {&lt;ancestor&gt;}                                                                                 unlink &lt;name&gt; from crush map (everywhere, or just at &lt;ancestor&gt;)
osd crush weight-set create &lt;poolname&gt; flat|positional                                                               create a weight-set for a given pool
osd crush weight-set create-compat                                                                                   create a default backward-compatible weight-set
osd crush weight-set dump                                                                                            dump crush weight sets
osd crush weight-set ls                                                                                              list crush weight sets
osd crush weight-set reweight &lt;poolname&gt; &lt;item&gt; &lt;float[0.0-]&gt; [&lt;float[0.0-]&gt;...]                                     set weight for an item (bucket or osd) in a pool's weight-set
osd crush weight-set reweight-compat &lt;item&gt; &lt;float[0.0-]&gt; [&lt;float[0.0-]&gt;...]                                         set weight for an item (bucket or osd) in the backward-compatible weight-set
osd crush weight-set rm &lt;poolname&gt;                                                                                   remove the weight-set for a given pool
osd crush weight-set rm-compat                                                                                       remove the backward-compatible weight-set
cephu@cephadmin:~/my-cluster$ ceph osd crush tunables hammer
adjusted tunables profile to hammer
cephu@cephadmin:~/my-cluster$ ceph osd crush show-tunables
{
    "choose_local_tries": 0,
    "choose_local_fallback_tries": 0,
    "choose_total_tries": 50,
    "chooseleaf_descend_once": 1,
    "chooseleaf_vary_r": 1,
    "chooseleaf_stable": 0,
    "straw_calc_version": 1,
    "allowed_bucket_algs": 54,
    "profile": "hammer",
    "optimal_tunables": 0,
    "legacy_tunables": 0,
    "minimum_required_version": "hammer",
    "require_feature_tunables": 1,
    "require_feature_tunables2": 1,
    "has_v2_rules": 0,
    "require_feature_tunables3": 1,
    "has_v3_rules": 0,
    "has_v4_buckets": 1,
    "require_feature_tunables5": 0,
    "has_v5_rules": 0
}

cephu@cephadmin:~/my-cluster$
</pre>

<p>
回 ceph-client 节点：
</p>

<pre class="example">
cephu@ceph-client:~$ ceph -v
ceph version 12.2.1 (3e7492b9ada8bdc9a5cd0feafd42fbca27f9c38e) luminous (stable)
cephu@ceph-client:~$ rbd ls
foo
cephu@ceph-client:~$ sudo rbd map foo --name client.admin
/dev/rbd0
cephu@ceph-client:~$ ls /dev/rbd
rbd/  rbd0
cephu@ceph-client:~$ ls /dev/rbd/rbd/foo
/dev/rbd/rbd/foo
cephu@ceph-client:~$ ls /dev/rbd0
/dev/rbd0
cephu@ceph-client:~$ ls /dev/rbd0 -l
brw-rw---- 1 root disk 251, 0 Oct 25 12:03 /dev/rbd0
cephu@ceph-client:~$
</pre>

<p>
成功了。。继续。。
</p>

<p>
Use the block device by creating a file system on the ceph-client node.
</p>

<pre class="example">
cephu@ceph-client:~$ sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo
mke2fs 1.42.13 (17-May-2015)
Discarding device blocks: done
Creating filesystem with 1048576 4k blocks and 262144 inodes
Filesystem UUID: d83ebc8d-1956-4d81-b9db-391f939634ac
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376, 294912, 819200, 884736

Allocating group tables: done
Writing inode tables: done
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done

cephu@ceph-client:~$
</pre>

<p>
Mount the file system on the ceph-client node.
</p>

<pre class="example">
cephu@ceph-client:~$ sudo mkdir /mnt/ceph-block-device
cephu@ceph-client:~$ sudo mount /dev/rbd/rbd/foo /mnt/ceph-block-device
cephu@ceph-client:~$ cd /mnt/ceph-block-device
cephu@ceph-client:/mnt/ceph-block-device$ ls
lost+found
cephu@ceph-client:/mnt/ceph-block-device$
</pre>

<p>
好了，这小一节，结束了。
</p>
</div>
</li>
</ul>
</div>

<div id="outline-container-org52e256d" class="outline-4">
<h4 id="org52e256d">ceph-install-base-ubuntu Filesystem Quick Start</h4>
<div class="outline-text-4" id="text-org52e256d">
<p>
<a href="http://docs.ceph.com/docs/master/start/quick-cephfs/">http://docs.ceph.com/docs/master/start/quick-cephfs/</a>
</p>
</div>

<ul class="org-ul">
<li><a id="prerequisites"></a>PREREQUISITES<br />
<div class="outline-text-5" id="text-prerequisites">
<p>
admin 节点：
</p>

<pre class="example">
ceph -s
</pre>

<p>
看一下，是不是都正常。 这里是看不到 mds 的，要看，要去 moniter 节点
</p>

<p>
moniter 节点： 我们环境的是 192.168.31.114
</p>

<pre class="example">
cephu@mon1:~$ ps -ef | grep ceph
root      7541  7227  0 Oct23 pts/0    00:00:00 su - cephu
cephu     7542  7541  0 Oct23 pts/0    00:00:00 -su
cephu    20793  7542  0 12:39 pts/0    00:00:00 ps -ef
cephu    20794  7542  0 12:39 pts/0    00:00:00 grep --color=auto ceph
ceph     21275     1  0 Oct23 ?        00:07:50 /usr/bin/ceph-mon -f --cluster ceph --id mon1 --setuser ceph --setgroup ceph
ceph     22756     1  0 Oct24 ?        00:03:30 /usr/bin/ceph-mgr -f --cluster ceph --id mon1 --setuser ceph --setgroup ceph
ceph     25536     1  0 Oct24 ?        00:04:25 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph
ceph     26870     1  0 Oct24 ?        00:00:30 /usr/bin/ceph-mds -f --cluster ceph --id node1 --setuser ceph --setgroup ceph
ceph     29545     1  0 Oct24 ?        00:03:06 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node1 --setuser ceph --setgroup ceph
cephu@mon1:~$
cephu@mon1:~$ ps -ef | grep ceph | grep mds
cephu    21152  7542  0 12:45 pts/0    00:00:00 grep --color=auto mds
ceph     26870     1  0 Oct24 ?        00:00:30 /usr/bin/ceph-mds -f --cluster ceph --id node1 --setuser ceph --setgroup ceph
cephu@mon1:~$
</pre>
</div>
</li>

<li><a id="create-a-filesystem"></a>CREATE A FILESYSTEM<br />
<div class="outline-text-5" id="text-create-a-filesystem">
<p>
admin 节点：
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph osd pool create cephfs_data 8
pool 'cephfs_data' created
cephu@cephadmin:~/my-cluster$ ceph osd pool create cephfs_metadata 8
pool 'cephfs_metadata' created
cephu@cephadmin:~/my-cluster$ ceph fs new cephfs-jlch cephfs_metadata cephfs_data
new fs with metadata pool 8 and data pool 7
cephu@cephadmin:~/my-cluster$ ceph fs ls
name: cephfs-jlch, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
cephu@cephadmin:~/my-cluster$
cephu@cephadmin:~/my-cluster$ ceph mds stat
cephfs-jlch-1/1/1 up  {0=node1=up:active}
cephu@cephadmin:~/my-cluster$
</pre>
</div>
</li>

<li><a id="create-a-secret-file"></a>CREATE A SECRET FILE<br />
<div class="outline-text-5" id="text-create-a-secret-file">
<p>
admin 节点：
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ cat ceph.client.admin.keyring
[client.admin]
        key = AQCtj+5ZnNTvGRAA2RxAGcIQZJnaJSPEz4jdGw==
cephu@cephadmin:~/my-cluster$ echo "AQCtj+5ZnNTvGRAA2RxAGcIQZJnaJSPEz4jdGw==" &gt; admin.secret
cephu@cephadmin:~/my-cluster$ ls
admin.secret                ceph.bootstrap-osd.keyring  ceph.conf             testfile.txt
ceph.bootstrap-mds.keyring  ceph.bootstrap-rgw.keyring  ceph-deploy-ceph.log
ceph.bootstrap-mgr.keyring  ceph.client.admin.keyring   ceph.mon.keyring
cephu@cephadmin:~/my-cluster$ cat admin.secret
AQCtj+5ZnNTvGRAA2RxAGcIQZJnaJSPEz4jdGw==
cephu@cephadmin:~/my-cluster$
</pre>
</div>
</li>

<li><a id="kernel-driver"></a>KERNEL DRIVER<br />
<div class="outline-text-5" id="text-kernel-driver">
<pre class="example">
cephu@cephadmin:~/my-cluster$ sudo mkdir /mnt/mycephfs
cephu@cephadmin:~/my-cluster$ sudo mount -t ceph 192.168.31.114:6789:/ /mnt/mycephfs
mount error 22 = Invalid argument
cephu@cephadmin:~/my-cluster$ sudo mount -t ceph 192.168.31.114:6789:/ /mnt/mycephfs -o name=admin,secretfile=admin.secret
cephu@cephadmin:~/my-cluster$
</pre>

<p>
如下可以看出， cephfs 将3个物理节点上的磁盘全部空间（82G = 16G + 16G +
50G）作为了自己的空间。
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ df -h
Filesystem                   Size  Used Avail Use% Mounted on
udev                         3.9G     0  3.9G   0% /dev
tmpfs                        799M   79M  720M  10% /run
/dev/mapper/ubuntu--vg-root   71G  4.7G   62G   8% /
tmpfs                        3.9G     0  3.9G   0% /dev/shm
tmpfs                        5.0M     0  5.0M   0% /run/lock
tmpfs                        3.9G     0  3.9G   0% /sys/fs/cgroup
/dev/loop0                    84M   84M     0 100% /snap/core/3017
/dev/loop2                    82M   82M     0 100% /snap/core/2898
/dev/sda1                    472M   58M  391M  13% /boot
tmpfs                        799M     0  799M   0% /run/user/1000
/dev/loop3                    84M   84M     0 100% /snap/core/3247
192.168.31.114:6789:/         82G  4.2G   78G   6% /mnt/mycephfs
cephu@cephadmin:~/my-cluster$
</pre>
</div>
</li>

<li><a id="filesystem-in-user-space-fuse"></a>FILESYSTEM IN USER SPACE (FUSE)<br />
<div class="outline-text-5" id="text-filesystem-in-user-space-fuse">
<pre class="example">
cephu@cephadmin:~/my-cluster$ sudo ceph-fuse -k ./ceph.client.admin.keyring -m 192.168.31.114:6789 ~/mycephfs
sudo: ceph-fuse: command not found
cephu@cephadmin:~/my-cluster$
</pre>

<p>
没有这个命令。
</p>

<p>
农总这个时候，让我去 ceph-client 节点
</p>

<p>
ceph-client 节点：
</p>

<pre class="example">
cephu@ceph-client:~$ which ceph-fuse
cephu@ceph-client:~$ apt search ceph-fuse
Sorting... Done
Full Text Search... Done
ceph-fuse/stable 10.2.10-1xenial amd64
FUSE-based client for the Ceph distributed file system

ceph-fuse-dbg/stable 10.2.10-1xenial amd64
debugging symbols for ceph-fuse

cephu@ceph-client:~$ sudo apt install ceph-fuse
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following packages were automatically installed and are no longer required:
libboost-program-options1.58.0 libboost-random1.58.0 libboost-regex1.58.0 libcephfs1 libfcgi0ldbl libllvm3.8
libmircommon5 linux-headers-4.4.0-31 linux-headers-4.4.0-31-generic linux-image-4.4.0-31-generic
linux-image-extra-4.4.0-31-generic
Use 'sudo apt autoremove' to remove them.
The following NEW packages will be installed:
ceph-fuse
0 upgraded, 1 newly installed, 0 to remove and 48 not upgraded.
Need to get 2,926 kB of archives.
After this operation, 7,873 kB of additional disk space will be used.
Get:1 https://download.ceph.com/debian-jewel xenial/main amd64 ceph-fuse amd64 10.2.10-1xenial [2,926 kB]
Fetched 2,926 kB in 20s (145 kB/s)
Selecting previously unselected package ceph-fuse.
(Reading database ... 152232 files and directories currently installed.)
Preparing to unpack .../ceph-fuse_10.2.10-1xenial_amd64.deb ...
Unpacking ceph-fuse (10.2.10-1xenial) ...
Processing triggers for man-db (2.7.5-1) ...
Setting up ceph-fuse (10.2.10-1xenial) ...

cephu@ceph-client:~$ sudo mkdir ~/mycephfs
cephu@ceph-client:~$ sudo ceph-fuse  -m 192.168.31.114:6789 ~/mycephfs
ceph-fuse[28422]: starting ceph client
2017-10-25 14:10:35.041539 7fb1f6430f00 -1 init, newargv = 0x5608ad4acf60 newargc=11
ceph-fuse[28422]: starting fuse
cephu@ceph-client:~$
</pre>

<p>
可以了，成功了。
</p>

<p>
看一下状态
</p>

<pre class="example">
cephu@ceph-client:~$ ceph mds stat
cephfs-jlch-1/1/1 up  {0=node1=up:active}
cephu@ceph-client:~$ df -h
Filesystem                   Size  Used Avail Use% Mounted on
udev                         3.9G     0  3.9G   0% /dev
tmpfs                        799M  9.0M  790M   2% /run
/dev/mapper/ubuntu--vg-root   35G  4.2G   29G  13% /
tmpfs                        3.9G  528K  3.9G   1% /dev/shm
tmpfs                        5.0M     0  5.0M   0% /run/lock
tmpfs                        3.9G     0  3.9G   0% /sys/fs/cgroup
/dev/sda1                    472M  153M  295M  35% /boot
tmpfs                        799M     0  799M   0% /run/user/113
tmpfs                        799M     0  799M   0% /run/user/1000
/dev/rbd0                    3.9G  8.0M  3.8G   1% /mnt/ceph-block-device
ceph-fuse                     82G  4.2G   78G   6% /home/cephu/mycephfs
cephu@ceph-client:~$
</pre>
</div>
</li>

<li><a id="additional-information"></a>ADDITIONAL INFORMATION<br />
<div class="outline-text-5" id="text-additional-information">
<p>
这里没有什么了。
</p>
</div>
</li>

<li><a id="测试一下"></a>测试一下<br />
<div class="outline-text-5" id="text-测试一下">
<p>
回到 admin 节点吧。
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ df -h
Filesystem                   Size  Used Avail Use% Mounted on
udev                         3.9G     0  3.9G   0% /dev
tmpfs                        799M   79M  720M  10% /run
/dev/mapper/ubuntu--vg-root   71G  4.7G   62G   8% /
tmpfs                        3.9G     0  3.9G   0% /dev/shm
tmpfs                        5.0M     0  5.0M   0% /run/lock
tmpfs                        3.9G     0  3.9G   0% /sys/fs/cgroup
/dev/loop0                    84M   84M     0 100% /snap/core/3017
/dev/loop2                    82M   82M     0 100% /snap/core/2898
/dev/sda1                    472M   58M  391M  13% /boot
tmpfs                        799M     0  799M   0% /run/user/1000
/dev/loop3                    84M   84M     0 100% /snap/core/3247
192.168.31.114:6789:/         82G  4.2G   78G   6% /mnt/mycephfs
cephu@cephadmin:~/my-cluster$ touch /mnt/mycephfs/h.txt
touch: cannot touch '/mnt/mycephfs/h.txt': Permission denied
cephu@cephadmin:~/my-cluster$
</pre>

<p>
为什么这样子？？？ Permission denied 这不是玩我么？
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ sudo touch /mnt/mycephfs/h.txt
cephu@cephadmin:~/my-cluster$ sudo vi /mnt/mycephfs/h.txt
cephu@cephadmin:~/my-cluster$ sudo ls -l /mnt/mycephfs/
total 1
-rw-r--r-- 1 root root 14 Oct 25 14:33 h.txt
</pre>

<p>
那直接修改目录权限。
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ sudo chown -R cephu:cephu /mnt/mycephfs/
cephu@cephadmin:~/my-cluster$ touch /mnt/mycephfs/a.txt
cephu@cephadmin:~/my-cluster$ echo "hello, world" &gt; /mnt/mycephfs/b.txt
cephu@cephadmin:~/my-cluster$ cat /mnt/mycephfs/b.txt
hello, world
cephu@cephadmin:~/my-cluster$ sudo ls -l /mnt/mycephfs/
total 1
-rw-rw-r-- 1 cephu cephu  0 Oct 25 14:34 a.txt
-rw-rw-r-- 1 cephu cephu  2 Oct 25 14:35 b.txt
-rw-r--r-- 1 cephu cephu 14 Oct 25 14:33 h.txt
cephu@cephadmin:~/my-cluster$
</pre>

<p>
好了。这下成功了。
</p>

<p>
再回到 ceph-client 下测试吧（因为其下的 mount 方式不同，是
ceph-fuse，也测试一下咯）。
</p>

<p>
ceph-client节点：
</p>

<pre class="example">
cephu@ceph-client:~$ ls mycephfs/
a.txt  b.txt  h.txt
</pre>

<p>
文件是看到了。 创建一下。
</p>

<pre class="example">
cephu@ceph-client:~$ echo "c" &gt; mycephfs/b.txt
cephu@ceph-client:~$ cat mycephfs/b.txt
c
cephu@ceph-client:~$
</pre>

<p>
OK。成功。
</p>
</div>
</li>

<li><a id="umount"></a>umount<br />
<div class="outline-text-5" id="text-umount">
<p>
到我们这个文档结束的时候，记得要 umount 一下哟。（这里只写
admin节点了，ceph-client节点是一样样的。）
</p>

<p>
admin:
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ sudo umount /mnt/mycephfs
</pre>

<p>
检查一下：
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ mount
sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
udev on /dev type devtmpfs (rw,nosuid,relatime,size=4067352k,nr_inodes=1016838,mode=755)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)
tmpfs on /run type tmpfs (rw,nosuid,noexec,relatime,size=817512k,mode=755)
/dev/mapper/ubuntu--vg-root on / type ext4 (rw,relatime,errors=remount-ro,data=ordered)
securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)
tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd)
pstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
systemd-1 on /proc/sys/fs/binfmt_misc type autofs (rw,relatime,fd=27,pgrp=1,timeout=0,minproto=5,maxproto=5,direct)
mqueue on /dev/mqueue type mqueue (rw,relatime)
hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime)
debugfs on /sys/kernel/debug type debugfs (rw,relatime)
fusectl on /sys/fs/fuse/connections type fusectl (rw,relatime)
/var/lib/snapd/snaps/core_3017.snap on /snap/core/3017 type squashfs (ro,nodev,relatime)
/var/lib/snapd/snaps/core_2898.snap on /snap/core/2898 type squashfs (ro,nodev,relatime)
/dev/sda1 on /boot type ext2 (rw,relatime,block_validity,barrier,user_xattr,acl)
lxcfs on /var/lib/lxcfs type fuse.lxcfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
/dev/mapper/ubuntu--vg-root on /var/lib/docker/aufs type ext4 (rw,relatime,errors=remount-ro,data=ordered)
tmpfs on /run/user/1000 type tmpfs (rw,nosuid,nodev,relatime,size=817512k,mode=700,uid=1000,gid=1000)
/var/lib/snapd/snaps/core_3247.snap on /snap/core/3247 type squashfs (ro,nodev,relatime)
tmpfs on /run/snapd/ns type tmpfs (rw,nosuid,noexec,relatime,size=817512k,mode=755)
nsfs on /run/snapd/ns/core.mnt type nsfs (rw)
cephu@cephadmin:~/my-cluster$
</pre>

<p>
确实没有了。安心。
</p>

<p>
到此，应该这一小节结束。 喝杯水，压压惊！~~~~~~~~~~
</p>
</div>
</li>
</ul>
</div>
<div id="outline-container-orgd2e8af4" class="outline-4">
<h4 id="orgd2e8af4">ceph-install-base-ubuntu Object Storage Quick Start</h4>
<div class="outline-text-4" id="text-orgd2e8af4">
</div>

<ul class="org-ul">
<li><a id="installing-ceph-object-gateway"></a>INSTALLING CEPH OBJECT GATEWAY<br />
<div class="outline-text-5" id="text-installing-ceph-object-gateway">
<p>
ceph-client：
</p>

<pre class="example">
cephu@ceph-client:~$ sudo ufw allow 7480
Rule added
Rule added (v6)
cephu@ceph-client:~$ sudo ufw status
Status: active

To                         Action      From
--                         ------      ----
22                         ALLOW       Anywhere
7480                       ALLOW       Anywhere
22 (v6)                    ALLOW       Anywhere (v6)
7480 (v6)                  ALLOW       Anywhere (v6)

cephu@ceph-client:~$
</pre>

<p>
ceph-admin:
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ping ceph-client
PING ceph-client (192.168.31.172) 56(84) bytes of data.
64 bytes from ceph-client (192.168.31.172): icmp_seq=1 ttl=64 time=0.558 ms
^C
--- ceph-client ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.558/0.558/0.558/0.000 ms
cephu@cephadmin:~/my-cluster$ ceph-deploy install --rgw ceph-client
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephu/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /home/cephu/.local/bin/ceph-deploy install --rgw ceph-client
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  testing                       : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f61e87c4908&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  dev_commit                    : None
[ceph_deploy.cli][INFO  ]  install_mds                   : False
[ceph_deploy.cli][INFO  ]  stable                        : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[ceph_deploy.cli][INFO  ]  func                          : &lt;function install at 0x7f61e8e9b848&gt;
[ceph_deploy.cli][INFO  ]  install_mgr                   : False
[ceph_deploy.cli][INFO  ]  install_all                   : False
[ceph_deploy.cli][INFO  ]  repo                          : False
[ceph_deploy.cli][INFO  ]  host                          : ['ceph-client']
[ceph_deploy.cli][INFO  ]  install_rgw                   : True
[ceph_deploy.cli][INFO  ]  install_tests                 : False
[ceph_deploy.cli][INFO  ]  repo_url                      : None
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  install_osd                   : False
[ceph_deploy.cli][INFO  ]  version_kind                  : stable
[ceph_deploy.cli][INFO  ]  install_common                : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  dev                           : master
[ceph_deploy.cli][INFO  ]  nogpgcheck                    : False
[ceph_deploy.cli][INFO  ]  local_mirror                  : None
[ceph_deploy.cli][INFO  ]  release                       : None
[ceph_deploy.cli][INFO  ]  install_mon                   : False
[ceph_deploy.cli][INFO  ]  gpg_url                       : None
[ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts ceph-client
[ceph_deploy.install][DEBUG ] Detecting platform for host ceph-client ...
[ceph-client][DEBUG ] connection detected need for sudo
[ceph-client][DEBUG ] connected to host: ceph-client
[ceph-client][DEBUG ] detect platform information from remote host
[ceph-client][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[ceph-client][INFO  ] installing Ceph on ceph-client
[ceph-client][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates apt-transport-https
[ceph-client][DEBUG ] Reading package lists...
[ceph-client][DEBUG ] Building dependency tree...
[ceph-client][DEBUG ] Reading state information...
[ceph-client][DEBUG ] apt-transport-https is already the newest version (1.2.24).
[ceph-client][DEBUG ] ca-certificates is already the newest version (20170717~16.04.1).
[ceph-client][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph-client][DEBUG ]   libboost-program-options1.58.0 libboost-random1.58.0 libboost-regex1.58.0
[ceph-client][DEBUG ]   libcephfs1 libfcgi0ldbl libllvm3.8 libmircommon5 linux-headers-4.4.0-31
[ceph-client][DEBUG ]   linux-headers-4.4.0-31-generic linux-image-4.4.0-31-generic
[ceph-client][DEBUG ]   linux-image-extra-4.4.0-31-generic
[ceph-client][DEBUG ] Use 'sudo apt autoremove' to remove them.
[ceph-client][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.
[ceph-client][INFO  ] Running command: sudo wget -O release.asc https://download.ceph.com/keys/release.asc
[ceph-client][WARNIN] --2017-10-25 14:45:28--  https://download.ceph.com/keys/release.asc
[ceph-client][WARNIN] Resolving download.ceph.com (download.ceph.com)... 158.69.68.124, 2607:5300:201:2000::3:58a1
[ceph-client][WARNIN] Connecting to download.ceph.com (download.ceph.com)|158.69.68.124|:443... connected.
[ceph-client][WARNIN] HTTP request sent, awaiting response... 200 OK
[ceph-client][WARNIN] Length: 1645 (1.6K) [application/octet-stream]
[ceph-client][WARNIN] Saving to: ‘release.asc’
[ceph-client][WARNIN]
[ceph-client][WARNIN]      0K .                                                     100%  233M=0s
[ceph-client][WARNIN]
[ceph-client][WARNIN] 2017-10-25 14:45:35 (233 MB/s) - ‘release.asc’ saved [1645/1645]
[ceph-client][WARNIN]
[ceph-client][INFO  ] Running command: sudo apt-key add release.asc
[ceph-client][DEBUG ] OK
[ceph-client][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[ceph-client][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[ceph-client][DEBUG ] Hit:1 http://cn.archive.ubuntu.com/ubuntu xenial InRelease
[ceph-client][DEBUG ] Hit:2 http://cn.archive.ubuntu.com/ubuntu xenial-updates InRelease
[ceph-client][DEBUG ] Hit:3 http://cn.archive.ubuntu.com/ubuntu xenial-backports InRelease
[ceph-client][DEBUG ] Hit:4 http://www.rabbitmq.com/debian testing InRelease
[ceph-client][DEBUG ] Get:5 http://security.ubuntu.com/ubuntu xenial-security InRelease [102 kB]
[ceph-client][DEBUG ] Hit:6 https://download.docker.com/linux/ubuntu xenial InRelease
[ceph-client][DEBUG ] Get:7 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [7,472 B]
[ceph-client][DEBUG ] Get:8 http://security.ubuntu.com/ubuntu xenial-security/restricted i386 Packages [7,472 B]
[ceph-client][DEBUG ] Get:9 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [3,208 B]
[ceph-client][DEBUG ] Get:10 http://security.ubuntu.com/ubuntu xenial-security/multiverse i386 Packages [3,384 B]
[ceph-client][DEBUG ] Hit:11 https://download.ceph.com/debian-jewel xenial InRelease
[ceph-client][DEBUG ] Fetched 124 kB in 7s (16.8 kB/s)
[ceph-client][DEBUG ] Reading package lists...
[ceph-client][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew radosgw
[ceph-client][DEBUG ] Reading package lists...
[ceph-client][DEBUG ] Building dependency tree...
[ceph-client][DEBUG ] Reading state information...
[ceph-client][DEBUG ] radosgw is already the newest version (12.2.1-1xenial).
[ceph-client][DEBUG ] The following packages were automatically installed and are no longer required:
[ceph-client][DEBUG ]   libboost-program-options1.58.0 libboost-random1.58.0 libboost-regex1.58.0
[ceph-client][DEBUG ]   libcephfs1 libfcgi0ldbl libllvm3.8 libmircommon5 linux-headers-4.4.0-31
[ceph-client][DEBUG ]   linux-headers-4.4.0-31-generic linux-image-4.4.0-31-generic
[ceph-client][DEBUG ]   linux-image-extra-4.4.0-31-generic
[ceph-client][DEBUG ] Use 'sudo apt autoremove' to remove them.
[ceph-client][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.
[ceph-client][INFO  ] Running command: sudo ceph --version
[ceph-client][DEBUG ] ceph version 12.2.1 (3e7492b9ada8bdc9a5cd0feafd42fbca27f9c38e) luminous (stable)
cephu@cephadmin:~/my-cluster$
</pre>

<p>
看状态
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph -s
cluster:
    id:     d0aa5af1-4f8e-4953-9448-7f1b2448b8a5
    health: HEALTH_OK

services:
    mon: 3 daemons, quorum cephfsn2,mon1,cephfsn3
    mgr: mon1(active), standbys: node2, node3
    mds: cephfs-jlch-1/1/1 up  {0=node1=up:active}
    osd: 3 osds: 3 up, 3 in
    rgw: 1 daemon active

data:
    pools:   8 pools, 64 pgs
    objects: 296 objects, 134 MB
    usage:   4286 MB used, 79378 MB / 83664 MB avail
    pgs:     64 active+clean

cephu@cephadmin:~/my-cluster$
</pre>
</div>
</li>

<li><a id="creating-the-ceph-object-gateway-instance"></a>CREATING THE CEPH OBJECT GATEWAY INSTANCE<br />
<div class="outline-text-5" id="text-creating-the-ceph-object-gateway-instance">
<p>
admin:
</p>

<pre class="example">
cephu@cephadmin:~/my-cluster$ ceph-deploy rgw create ceph-client
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephu/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /home/cephu/.local/bin/ceph-deploy rgw create ceph-client
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  rgw                           : [('ceph-client', 'rgw.ceph-client')]
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1dc4631bd8&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : &lt;function rgw at 0x7f1dc4c7e140&gt;
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts ceph-client:rgw.ceph-client
[ceph-client][DEBUG ] connection detected need for sudo
[ceph-client][DEBUG ] connected to host: ceph-client
[ceph-client][DEBUG ] detect platform information from remote host
[ceph-client][DEBUG ] detect machine type
[ceph_deploy.rgw][INFO  ] Distro info: Ubuntu 16.04 xenial
[ceph_deploy.rgw][DEBUG ] remote host will use systemd
[ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to ceph-client
[ceph-client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.rgw][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 RGWs

cephu@cephadmin:~/my-cluster$ ceph-deploy --overwrite-conf rgw create ceph-client
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephu/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /home/cephu/.local/bin/ceph-deploy --overwrite-conf rgw create ceph-client
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  rgw                           : [('ceph-client', 'rgw.ceph-client')]
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb62d844bd8&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : &lt;function rgw at 0x7fb62de91140&gt;
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts ceph-client:rgw.ceph-client
[ceph-client][DEBUG ] connection detected need for sudo
[ceph-client][DEBUG ] connected to host: ceph-client
[ceph-client][DEBUG ] detect platform information from remote host
[ceph-client][DEBUG ] detect machine type
[ceph_deploy.rgw][INFO  ] Distro info: Ubuntu 16.04 xenial
[ceph_deploy.rgw][DEBUG ] remote host will use systemd
[ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to ceph-client
[ceph-client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph-client][WARNIN] rgw keyring does not exist yet, creating one
[ceph-client][DEBUG ] create a keyring file
[ceph-client][DEBUG ] create path recursively if it doesn't exist
[ceph-client][INFO  ] Running command: sudo ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.ceph-client osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.ceph-client/keyring
[ceph-client][INFO  ] Running command: sudo systemctl enable ceph-radosgw@rgw.ceph-client
[ceph-client][WARNIN] Created symlink from /etc/systemd/system/ceph-radosgw.target.wants/ceph-radosgw@rgw.ceph-client.service to /lib/systemd/system/ceph-radosgw@.service.
[ceph-client][INFO  ] Running command: sudo systemctl start ceph-radosgw@rgw.ceph-client
[ceph-client][INFO  ] Running command: sudo systemctl enable ceph.target
[ceph_deploy.rgw][INFO  ] The Ceph Object Gateway (RGW) is now running on host ceph-client and default port 7480
cephu@cephadmin:~/my-cluster$
</pre>
</div>
</li>

<li><a id="configuring-the-ceph-object-gateway-instance"></a>CONFIGURING THE CEPH OBJECT GATEWAY INSTANCE<br />
<div class="outline-text-5" id="text-configuring-the-ceph-object-gateway-instance">
<p>
这里是配置web端口成 80 .
</p>

<p>
这里我就不做了，我没必要。修改也简单，看一下就明白了。
</p>

<p>
我的 ceph-client 的IP是 192.168.31.172，则： 我就直接到浏览器下打开
<a href="http://192.168.31.172:7480/">http://192.168.31.172:7480/</a>
</p>

<p>
输出大体如下：
</p>

<pre class="example">
This XML file does not appear to have any style information associated with it. The document tree is shown below.
&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;
    &lt;Owner&gt;
        &lt;ID&gt;anonymous&lt;/ID&gt;
        &lt;DisplayName/&gt;
    &lt;/Owner&gt;
    &lt;Buckets/&gt;
&lt;/ListAllMyBucketsResult&gt;
</pre>

<p>
好了，这一小节就到这里了。
</p>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
</body>
</html>
